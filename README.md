# MSCS_634_Lab_2

  The purpose to this lab was to perform K-Nearest Neighbors (KNN) and Radius Neighbors (RNN) algorithm on the wine dataset that we got from sklearn library. This dataset consisted of 178 entries and 14 attributes which included alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline, and target. 
  We first split the dataset into 80% training set and 20% testing set and then we trained the dataset using the training set and then performed the KNN and RNN model on the data, we used the k values of 1,5, 11, 15, and 21 on the KNN model. We found that the model shows better perforamce when the k calues is 1 or 21, both achiving an accuracy of ~0.78. This suggests that the model performs consistently well whether it considers only a few neighbos or many, indicating that its accuracy is relatively insensitive to the specific number of neighbors chosen.
  We then perforamed the RNN model with these radius values: 350, 400, 450, 500, 550, and 600 on the dataset and found that a radius of 350 yields the highest accuracy at 0.75, while larger radius values result in slightly lower accuracies around 0.72. This suggests that a radius of 350 captures a well balanced set of relevant neighbors, whereas larger radii likely introduces noice by including neighbors from different classes.
  Overall, I think the KNN algorithm performed better than RNN when is came down to the accuracy and performs well even when the number of neighbors changes, where in RNN algorithm it was more sensitive to the radius size.
  I didn't really face any challenges while doing this lab. 
